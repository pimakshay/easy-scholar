{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying from a pdf\n",
    "\n",
    "- In this notebook, we load a random pdf and try to query from it.\n",
    "- We use ollama open-source models: `gemma2:2b`, `phi3.5` or `llama3.1:8b`\n",
    "- Embedding model: `nomic-embed-text`\n",
    "- two-step Retriever: ChromaRetriever (text retriever), `BM25Retriever` (metadata retriever), `EnsembleRetriever` (ensemble both)\n",
    "    - There is other retreiver called `SelfQueryRetriever` (not explored yet) which combines both in one model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Grobid Installation and Setup\n",
    "For using Grobid, you need the Grobid server running. Follow the below steps to pull Grobid image and run it on docker:\n",
    "1. `docker pull grobid/grobid:0.8.1-name-address`\n",
    "2. `docker run --rm --init --ulimit core=0 -p 8070:8070 grobid/grobid:0.8.1-name-address`\n",
    "\n",
    "\n",
    "## Step 2: Ollama Setup/Commands [Linux]\n",
    "\n",
    "Starting and stopping service\n",
    "1. Starting ollama service: `systemctl start ollama.service`\n",
    "2. Stopping ollama service: `systemctl stop ollama.service`\n",
    "3. Status of ollama service: `systemctl status ollama.service`\n",
    "\n",
    "Loading models\n",
    "1. pull the gemma2:2.b model:  `ollama pull gemma2:2b`\n",
    "2. run gemma model: `ollama run gemma2:2b`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear cache\n",
    "import torch\n",
    "torch.cuda.empty_cache() \n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# set system path\n",
    "CURR_DIR = os.path.dirname('__file__')\n",
    "ROOT_DIR=os.path.join(os.getcwd() ,'../')\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from langchain_community.document_loaders.generic import GenericLoader\n",
    "# from langchain_community.document_loaders.parsers import GrobidParser\n",
    "from src.retrieval.grobid_services import GrobidDocumentParser\n",
    "pdf_file_path = '../data/open_vocab_vit_object_detection.pdf' # Path to the pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(\n",
    "    path=pdf_file_path,\n",
    "    parser=GrobidDocumentParser(segment_sentences=False, ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    "    temperature=0.0,    \n",
    ")\n",
    "docSearch = Chroma.from_documents(texts, embedding=embeddings)\n",
    "docRetreiver = docSearch.as_retriever(search_type=\"similarity_score_threshold\", # or \"mmr\"\n",
    "                                      search_kwargs={#\"k\":3, \n",
    "                                                    #  \"lambda_mult\": 0.2, \n",
    "                                                    #  \"fetch_k\": 20,\n",
    "                                                     \"score_threshold\": 0.8})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "keyword_retriever = BM25Retriever.from_documents(texts)\n",
    "keyword_retriever.k =  3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_retriever = EnsembleRetriever(retrievers=[docRetreiver,\n",
    "                                                   keyword_retriever],\n",
    "                                       weights=[0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm_model = Ollama(\n",
    "    model=\"phi3.5:3.8b\", #\"llama3.1\", #gemma2:2b\", # llamma3.1:8b, phi3.5:3.8b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"\n",
    "You are an expert research scientist with a deep understanding of complex research topics. Your role is to analyze and explain intricate concepts from academic papers clearly and concisely. \n",
    "Focus on providing insightful summaries and elucidations that make the research accessible and understandable to a diverse audience, including those who may not have a scientific background.\n",
    "\"\"\"\n",
    "from langchain_community.llms import Ollama\n",
    "llm_model = Ollama(\n",
    "    model=\"llama3.1\",  #options: \"llama3.1\", gemma2:2b\", llamma3.1:8b, phi3.5:3.8b\n",
    "    temperature=0.0,  # Set the sampling temperature\n",
    "    num_predict=1000,  # Set the maximum number of tokens to generate\n",
    "    system=SYSTEM_MESSAGE,  # Set a system message\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ToDo: this part is not yet explored completely\n",
    "\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from src.retrieval.metadata_info import MetadataInfo\n",
    "\n",
    "paper_title = \"Simple Open-Vocabulary Object Detection with Vision Transformers\"\n",
    "\n",
    "metadata_info = MetadataInfo(document_content_des=f\"The document is a research paper titled {paper_title}\")\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm=llm_model,\n",
    "    vectorstore=docSearch,\n",
    "    document_contents=metadata_info.document_content_description,\n",
    "    metadata_field_info=metadata_info.metadata_field_info,\n",
    "    verbose=True,\n",
    "    use_original_query=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_model, chain_type=\"stuff\", \n",
    "                                 retriever=ensemble_retriever, \n",
    "                                 verbose=True, return_source_documents=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay/miniconda3/envs/vlm/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'bboxes': \"[[{'page': '3', 'x': '134.77', 'y': '644.13', 'h': '269.63', 'w': '8.77'}], [{'page': '3', 'x': '410.51', 'y': '644.16', 'h': '70.08', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '656.12', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '118.99', 'h': '218.68', 'w': '8.74'}], [{'page': '4', 'x': '357.63', 'y': '118.99', 'h': '122.95', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '130.95', 'h': '345.82', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '142.90', 'h': '297.28', 'w': '8.74'}], [{'page': '4', 'x': '436.95', 'y': '142.90', 'h': '43.64', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '154.86', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '166.81', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '178.77', 'h': '52.12', 'w': '8.74'}], [{'page': '4', 'x': '190.63', 'y': '178.77', 'h': '289.96', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '190.72', 'h': '322.59', 'w': '8.74'}], [{'page': '4', 'x': '461.74', 'y': '190.72', 'h': '18.85', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '202.68', 'h': '164.21', 'w': '8.74'}], [{'page': '4', 'x': '303.31', 'y': '202.65', 'h': '177.28', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '214.64', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '226.59', 'h': '141.61', 'w': '8.74'}], [{'page': '4', 'x': '280.17', 'y': '226.59', 'h': '200.42', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '238.55', 'h': '345.82', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '250.50', 'h': '72.00', 'w': '8.74'}], [{'page': '4', 'x': '210.25', 'y': '250.47', 'h': '270.34', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '262.46', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '274.41', 'h': '147.44', 'w': '8.74'}], [{'page': '4', 'x': '284.51', 'y': '274.41', 'h': '196.08', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '286.37', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '298.32', 'h': '262.47', 'w': '8.74'}], [{'page': '4', 'x': '399.92', 'y': '298.29', 'h': '80.68', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '310.25', 'h': '345.83', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '322.23', 'h': '133.72', 'w': '8.74'}], [{'page': '4', 'x': '271.70', 'y': '322.23', 'h': '208.90', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '334.19', 'h': '149.69', 'w': '8.74'}], [{'page': '4', 'x': '287.38', 'y': '334.19', 'h': '193.22', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '346.14', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '358.10', 'h': '129.23', 'w': '8.74'}], [{'page': '4', 'x': '267.18', 'y': '358.07', 'h': '213.41', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '370.05', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '382.01', 'h': '345.83', 'w': '8.74'}], [{'page': '4', 'x': '134.77', 'y': '393.96', 'h': '259.74', 'w': '8.74'}], [{'page': '4', 'x': '397.26', 'y': '393.96', 'h': '83.33', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '405.92', 'h': '345.82', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '417.87', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '429.83', 'h': '92.72', 'w': '8.74'}], [{'page': '4', 'x': '230.15', 'y': '429.80', 'h': '250.44', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '441.78', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '453.74', 'h': '294.07', 'w': '8.74'}], [{'page': '4', 'x': '149.71', 'y': '466.44', 'h': '330.88', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '478.40', 'h': '228.01', 'w': '8.74'}], [{'page': '4', 'x': '366.26', 'y': '478.40', 'h': '114.33', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '490.35', 'h': '345.82', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '502.31', 'h': '114.20', 'w': '8.74'}]]\", 'file_path': '../data/open_vocab_vit_object_detection.pdf', 'pages': \"('3', '4')\", 'paper_title': 'Simple Open-Vocabulary Object Detection with Vision Transformers', 'para': '18', 'section_number': '2', 'section_title': 'Related Work', 'text': \"Long-Tailed and Open-Vocabulary Object Detection.To go beyond a closed vocabulary, fixed classification layers can be replaced by language em-beddings to create open-vocabulary detectors [2].Open-vocabulary object detection has recently seen much progress from combining contrastively trained image-text models and classic object detectors [12,20,26,45,46,42].The main challenge in this task is how to transfer the image-level representations of the image-text backbone to detection despite the scarcity of localized annotations for rare classes.Making efficient use of the image-text pre-training is crucial since it allows for scaling without the need for expensive human annotations.Various approaches have been proposed.ViLD [12] distills embeddings obtained by applying CLIP or ALIGN to cropped image regions from a class-agnostic region proposal network (RPN).The RPN, however, limits generalization performance on novel objects, which is exacerbated by ViLD's two-step distillationtraining process.Multistage training is also used by RegionCLIP, which generates pseudo-labels on captioning data, followed by region-text contrastive pretraining, and transfer to detection.In contrast, our method fine-tunes both image and text models end-to-end on publicly available detection datasets, which simplifies training and improves generalization to unseen classes.MDETR [20] and GLIP [26] use a single text query for the whole image and formulate detection as the phrase grounding problem.This limits the number of object categories that can be processed per forward pass.Our architecture is simpler and more flexible in that it performs no image-text fusion and can handle multiple independent text or image-derived queries.OVR-CNN [42] is most similar to our approach in that it fine-tunes an image-text model to detection on a limited vocabulary and relies on image-text pre-training for generalization to an open vocabulary.However, we differ in all modelling and loss function choices.We use ViT [22] instead of their ResNet [15], a DETR-like model instead of their Faster-RCNN [34] and image-text pre-training as in LiT [44] instead of their PixelBERT [18] and visual grounding loss.Orthogonal to our approach, Detic [46] improves long-tail detection performance with weak supervision by training only the classification head on examples where only image-level annotations are available.We note that in our definition of open-vocabulary detection, object categories may overlap between detection training and testing.When we specifically refer to detecting categories for which no localized instances were seen during training, we use the term zero-shot.\"}, page_content=\"Long-Tailed and Open-Vocabulary Object Detection.To go beyond a closed vocabulary, fixed classification layers can be replaced by language em-beddings to create open-vocabulary detectors [2].Open-vocabulary object detection has recently seen much progress from combining contrastively trained image-text models and classic object detectors [12,20,26,45,46,42].The main challenge in this task is how to transfer the image-level representations of the image-text backbone to detection despite the scarcity of localized annotations for rare classes.Making efficient use of the image-text pre-training is crucial since it allows for scaling without the need for expensive human annotations.Various approaches have been proposed.ViLD [12] distills embeddings obtained by applying CLIP or ALIGN to cropped image regions from a class-agnostic region proposal network (RPN).The RPN, however, limits generalization performance on novel objects, which is exacerbated by ViLD's two-step distillationtraining process.Multistage training is also used by RegionCLIP, which generates pseudo-labels on captioning data, followed by region-text contrastive pretraining, and transfer to detection.In contrast, our method fine-tunes both image and text models end-to-end on publicly available detection datasets, which simplifies training and improves generalization to unseen classes.MDETR [20] and GLIP [26] use a single text query for the whole image and formulate detection as the phrase grounding problem.This limits the number of object categories that can be processed per forward pass.Our architecture is simpler and more flexible in that it performs no image-text fusion and can handle multiple independent text or image-derived queries.OVR-CNN [42] is most similar to our approach in that it fine-tunes an image-text model to detection on a limited vocabulary and relies on image-text pre-training for generalization to an open vocabulary.However, we differ in all modelling and loss function choices.We use ViT [22] instead of their ResNet [15], a DETR-like model instead of their Faster-RCNN [34] and image-text pre-training as in LiT [44] instead of their PixelBERT [18] and visual grounding loss.Orthogonal to our approach, Detic [46] improves long-tail detection performance with weak supervision by training only the classification head on examples where only image-level annotations are available.We note that in our definition of open-vocabulary detection, object categories may overlap between detection training and testing.When we specifically refer to detecting categories for which no localized instances were seen during training, we use the term zero-shot.\"), -173.99914218262106), (Document(metadata={'bboxes': \"[[{'page': '5', 'x': '134.77', 'y': '212.17', 'h': '345.82', 'w': '8.74'}, {'page': '5', 'x': '134.77', 'y': '224.12', 'h': '82.24', 'w': '8.74'}]]\", 'file_path': '../data/open_vocab_vit_object_detection.pdf', 'pages': \"('5', '5')\", 'paper_title': 'Simple Open-Vocabulary Object Detection with Vision Transformers', 'para': '0', 'section_number': '3', 'section_title': 'Method', 'text': 'The model can then be queried in different ways to perform open-vocabulary or few-shot detection.'}, page_content='The model can then be queried in different ways to perform open-vocabulary or few-shot detection.'), -177.65190501887594), (Document(metadata={'bboxes': \"[[{'page': '6', 'x': '134.77', 'y': '381.12', 'h': '111.83', 'w': '8.77'}], [{'page': '6', 'x': '251.21', 'y': '381.15', 'h': '229.38', 'w': '8.74'}, {'page': '6', 'x': '134.77', 'y': '393.10', 'h': '102.70', 'w': '8.74'}], [{'page': '6', 'x': '241.33', 'y': '393.10', 'h': '239.27', 'w': '8.74'}, {'page': '6', 'x': '134.77', 'y': '405.06', 'h': '293.00', 'w': '8.74'}], [{'page': '6', 'x': '431.65', 'y': '405.06', 'h': '48.94', 'w': '8.74'}, {'page': '6', 'x': '134.77', 'y': '417.01', 'h': '303.52', 'w': '8.74'}], [{'page': '6', 'x': '442.26', 'y': '417.01', 'h': '38.33', 'w': '8.74'}, {'page': '6', 'x': '134.77', 'y': '428.97', 'h': '316.13', 'w': '8.74'}]]\", 'file_path': '../data/open_vocab_vit_object_detection.pdf', 'pages': \"('6', '6')\", 'paper_title': 'Simple Open-Vocabulary Object Detection with Vision Transformers', 'para': '4', 'section_number': '3.2', 'section_title': 'Training', 'text': 'Training the Detector.Fine-tuning of pre-trained models for classification is a well-studied problem.Classifiers, especially large Transformers, require carefully tuned regularization and data augmentation to perform well.Recipes for classifier training are now well established in the literature [39,38,3].Here, we aim to provide a similar fine-tuning recipe for open-vocabulary detection.'}, page_content='Training the Detector.Fine-tuning of pre-trained models for classification is a well-studied problem.Classifiers, especially large Transformers, require carefully tuned regularization and data augmentation to perform well.Recipes for classifier training are now well established in the literature [39,38,3].Here, we aim to provide a similar fine-tuning recipe for open-vocabulary detection.'), -179.4167372024868), (Document(metadata={'bboxes': \"[[{'page': '2', 'x': '149.71', 'y': '498.47', 'h': '333.88', 'w': '8.74'}, {'page': '2', 'x': '134.77', 'y': '510.42', 'h': '77.24', 'w': '8.74'}], [{'page': '2', 'x': '215.32', 'y': '510.42', 'h': '200.49', 'w': '9.65'}]]\", 'file_path': '../data/open_vocab_vit_object_detection.pdf', 'pages': \"('2', '2')\", 'paper_title': 'Simple Open-Vocabulary Object Detection with Vision Transformers', 'para': '1', 'section_number': '1', 'section_title': 'Introduction', 'text': 'For open-vocabulary text-conditioned detection, our model achieves 34.6% AP overall and 31.2%AP rare on unseen classes on the LVIS dataset.'}, page_content='For open-vocabulary text-conditioned detection, our model achieves 34.6% AP overall and 31.2%AP rare on unseen classes on the LVIS dataset.'), -180.3712709353666)]\n",
      "  warnings.warn(\n",
      "/home/akshay/miniconda3/envs/vlm/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.8\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is open vocabulory detection as explained in the paper?',\n",
       " 'result': 'According to the paper, Open-Vocabulary Object Detection refers to the ability of a model to detect object categories that were not seen during training. This means that the model can recognize and classify objects even if they are not part of the fixed set of semantic categories used in traditional detection models.\\n\\nIn other words, open-vocabulary detection allows a model to generalize beyond a closed vocabulary, where the vocabulary refers to the specific set of object categories that were trained on. The goal is to enable the model to detect and classify objects from an open-ended or dynamic set of categories, without requiring explicit annotations for each category.',\n",
       " 'source_documents': [Document(metadata={'text': 'Object detection is a fundamental task in computer vision.Until recently, detection models were typically limited to a small, fixed set of semantic categories, because obtaining localized training data with large or open label spaces is costly and time-consuming.This has changed with the development of powerful language encoders and contrastive image-text training.These models learn a shared representation of image and text from loosely aligned image-text pairs, which are abundantly available on the web.By leveraging large amounts of image-text data, contrastive training has yielded major improvements in zero-shot classification performance and other language-based tasks [33,19,44].', 'para': '4', 'bboxes': \"[[{'page': '1', 'x': '134.77', 'y': '529.30', 'h': '262.28', 'w': '8.74'}], [{'page': '1', 'x': '400.99', 'y': '529.30', 'h': '79.59', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '541.25', 'h': '345.82', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '553.21', 'h': '345.82', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '565.16', 'h': '91.49', 'w': '8.74'}], [{'page': '1', 'x': '230.24', 'y': '565.16', 'h': '250.35', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '577.12', 'h': '222.16', 'w': '8.74'}], [{'page': '1', 'x': '359.53', 'y': '577.12', 'h': '121.06', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '589.07', 'h': '345.83', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '601.03', 'h': '159.38', 'w': '8.74'}], [{'page': '1', 'x': '297.19', 'y': '601.03', 'h': '183.40', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '612.98', 'h': '345.83', 'w': '8.74'}, {'page': '1', 'x': '134.77', 'y': '624.94', 'h': '270.63', 'w': '8.74'}]]\", 'pages': \"('1', '1')\", 'section_title': 'Introduction', 'section_number': '1', 'paper_title': 'Simple Open-Vocabulary Object Detection with Vision Transformers', 'file_path': '../data/open_vocab_vit_object_detection.pdf'}, page_content='Object detection is a fundamental task in computer vision.Until recently, detection models were typically limited to a small, fixed set of semantic categories, because obtaining localized training data with large or open label spaces is costly and time-consuming.This has changed with the development of powerful language encoders and contrastive image-text training.These models learn a shared representation of image and text from loosely aligned image-text pairs, which are abundantly available on the web.By leveraging large amounts of image-text data, contrastive training has yielded major improvements in zero-shot classification performance and other language-based tasks [33,19,44].'),\n",
       "  Document(metadata={'text': \"Long-Tailed and Open-Vocabulary Object Detection.To go beyond a closed vocabulary, fixed classification layers can be replaced by language em-beddings to create open-vocabulary detectors [2].Open-vocabulary object detection has recently seen much progress from combining contrastively trained image-text models and classic object detectors [12,20,26,45,46,42].The main challenge in this task is how to transfer the image-level representations of the image-text backbone to detection despite the scarcity of localized annotations for rare classes.Making efficient use of the image-text pre-training is crucial since it allows for scaling without the need for expensive human annotations.Various approaches have been proposed.ViLD [12] distills embeddings obtained by applying CLIP or ALIGN to cropped image regions from a class-agnostic region proposal network (RPN).The RPN, however, limits generalization performance on novel objects, which is exacerbated by ViLD's two-step distillationtraining process.Multistage training is also used by RegionCLIP, which generates pseudo-labels on captioning data, followed by region-text contrastive pretraining, and transfer to detection.In contrast, our method fine-tunes both image and text models end-to-end on publicly available detection datasets, which simplifies training and improves generalization to unseen classes.MDETR [20] and GLIP [26] use a single text query for the whole image and formulate detection as the phrase grounding problem.This limits the number of object categories that can be processed per forward pass.Our architecture is simpler and more flexible in that it performs no image-text fusion and can handle multiple independent text or image-derived queries.OVR-CNN [42] is most similar to our approach in that it fine-tunes an image-text model to detection on a limited vocabulary and relies on image-text pre-training for generalization to an open vocabulary.However, we differ in all modelling and loss function choices.We use ViT [22] instead of their ResNet [15], a DETR-like model instead of their Faster-RCNN [34] and image-text pre-training as in LiT [44] instead of their PixelBERT [18] and visual grounding loss.Orthogonal to our approach, Detic [46] improves long-tail detection performance with weak supervision by training only the classification head on examples where only image-level annotations are available.We note that in our definition of open-vocabulary detection, object categories may overlap between detection training and testing.When we specifically refer to detecting categories for which no localized instances were seen during training, we use the term zero-shot.\", 'para': '18', 'bboxes': \"[[{'page': '3', 'x': '134.77', 'y': '644.13', 'h': '269.63', 'w': '8.77'}], [{'page': '3', 'x': '410.51', 'y': '644.16', 'h': '70.08', 'w': '8.74'}, {'page': '3', 'x': '134.77', 'y': '656.12', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '118.99', 'h': '218.68', 'w': '8.74'}], [{'page': '4', 'x': '357.63', 'y': '118.99', 'h': '122.95', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '130.95', 'h': '345.82', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '142.90', 'h': '297.28', 'w': '8.74'}], [{'page': '4', 'x': '436.95', 'y': '142.90', 'h': '43.64', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '154.86', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '166.81', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '178.77', 'h': '52.12', 'w': '8.74'}], [{'page': '4', 'x': '190.63', 'y': '178.77', 'h': '289.96', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '190.72', 'h': '322.59', 'w': '8.74'}], [{'page': '4', 'x': '461.74', 'y': '190.72', 'h': '18.85', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '202.68', 'h': '164.21', 'w': '8.74'}], [{'page': '4', 'x': '303.31', 'y': '202.65', 'h': '177.28', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '214.64', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '226.59', 'h': '141.61', 'w': '8.74'}], [{'page': '4', 'x': '280.17', 'y': '226.59', 'h': '200.42', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '238.55', 'h': '345.82', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '250.50', 'h': '72.00', 'w': '8.74'}], [{'page': '4', 'x': '210.25', 'y': '250.47', 'h': '270.34', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '262.46', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '274.41', 'h': '147.44', 'w': '8.74'}], [{'page': '4', 'x': '284.51', 'y': '274.41', 'h': '196.08', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '286.37', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '298.32', 'h': '262.47', 'w': '8.74'}], [{'page': '4', 'x': '399.92', 'y': '298.29', 'h': '80.68', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '310.25', 'h': '345.83', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '322.23', 'h': '133.72', 'w': '8.74'}], [{'page': '4', 'x': '271.70', 'y': '322.23', 'h': '208.90', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '334.19', 'h': '149.69', 'w': '8.74'}], [{'page': '4', 'x': '287.38', 'y': '334.19', 'h': '193.22', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '346.14', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '358.10', 'h': '129.23', 'w': '8.74'}], [{'page': '4', 'x': '267.18', 'y': '358.07', 'h': '213.41', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '370.05', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '382.01', 'h': '345.83', 'w': '8.74'}], [{'page': '4', 'x': '134.77', 'y': '393.96', 'h': '259.74', 'w': '8.74'}], [{'page': '4', 'x': '397.26', 'y': '393.96', 'h': '83.33', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '405.92', 'h': '345.82', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '417.87', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '429.83', 'h': '92.72', 'w': '8.74'}], [{'page': '4', 'x': '230.15', 'y': '429.80', 'h': '250.44', 'w': '8.77'}, {'page': '4', 'x': '134.77', 'y': '441.78', 'h': '345.83', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '453.74', 'h': '294.07', 'w': '8.74'}], [{'page': '4', 'x': '149.71', 'y': '466.44', 'h': '330.88', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '478.40', 'h': '228.01', 'w': '8.74'}], [{'page': '4', 'x': '366.26', 'y': '478.40', 'h': '114.33', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '490.35', 'h': '345.82', 'w': '8.74'}, {'page': '4', 'x': '134.77', 'y': '502.31', 'h': '114.20', 'w': '8.74'}]]\", 'pages': \"('3', '4')\", 'section_title': 'Related Work', 'section_number': '2', 'paper_title': 'Simple Open-Vocabulary Object Detection with Vision Transformers', 'file_path': '../data/open_vocab_vit_object_detection.pdf'}, page_content=\"Long-Tailed and Open-Vocabulary Object Detection.To go beyond a closed vocabulary, fixed classification layers can be replaced by language em-beddings to create open-vocabulary detectors [2].Open-vocabulary object detection has recently seen much progress from combining contrastively trained image-text models and classic object detectors [12,20,26,45,46,42].The main challenge in this task is how to transfer the image-level representations of the image-text backbone to detection despite the scarcity of localized annotations for rare classes.Making efficient use of the image-text pre-training is crucial since it allows for scaling without the need for expensive human annotations.Various approaches have been proposed.ViLD [12] distills embeddings obtained by applying CLIP or ALIGN to cropped image regions from a class-agnostic region proposal network (RPN).The RPN, however, limits generalization performance on novel objects, which is exacerbated by ViLD's two-step distillationtraining process.Multistage training is also used by RegionCLIP, which generates pseudo-labels on captioning data, followed by region-text contrastive pretraining, and transfer to detection.In contrast, our method fine-tunes both image and text models end-to-end on publicly available detection datasets, which simplifies training and improves generalization to unseen classes.MDETR [20] and GLIP [26] use a single text query for the whole image and formulate detection as the phrase grounding problem.This limits the number of object categories that can be processed per forward pass.Our architecture is simpler and more flexible in that it performs no image-text fusion and can handle multiple independent text or image-derived queries.OVR-CNN [42] is most similar to our approach in that it fine-tunes an image-text model to detection on a limited vocabulary and relies on image-text pre-training for generalization to an open vocabulary.However, we differ in all modelling and loss function choices.We use ViT [22] instead of their ResNet [15], a DETR-like model instead of their Faster-RCNN [34] and image-text pre-training as in LiT [44] instead of their PixelBERT [18] and visual grounding loss.Orthogonal to our approach, Detic [46] improves long-tail detection performance with weak supervision by training only the classification head on examples where only image-level annotations are available.We note that in our definition of open-vocabulary detection, object categories may overlap between detection training and testing.When we specifically refer to detecting categories for which no localized instances were seen during training, we use the term zero-shot.\"),\n",
       "  Document(metadata={'text': 'Table 2. One-and few-shot image-conditioned detection performance on COCO AP50.Our method (R50+H/32 architecture) strongly outperforms prior work and also shows marked improvements as the number of conditioning queries is increased to k = 10.COCO category splits as in [16].Because the evaluation is stochastic, for our results, we report the average across 3 runs.For evaluation on this task, we follow the procedure described in [16]: During detection training, we hold out some COCO categories to evaluate on, and in addition all synonymous and semantically descendant categories that appear in our detection training data.We do not modify the image-text pre-training stage.', 'para': '5', 'bboxes': \"[[{'page': '10', 'x': '134.77', 'y': '115.91', 'h': '345.83', 'w': '7.89'}], [{'page': '10', 'x': '134.77', 'y': '126.90', 'h': '345.82', 'w': '7.86'}, {'page': '10', 'x': '134.77', 'y': '137.86', 'h': '345.82', 'w': '7.86'}], [{'page': '10', 'x': '134.77', 'y': '148.81', 'h': '131.99', 'w': '7.86'}], [{'page': '10', 'x': '270.04', 'y': '148.81', 'h': '210.55', 'w': '7.86'}, {'page': '10', 'x': '134.77', 'y': '159.77', 'h': '145.42', 'w': '7.86'}], [{'page': '10', 'x': '149.71', 'y': '330.44', 'h': '330.88', 'w': '8.74'}, {'page': '10', 'x': '134.77', 'y': '342.39', 'h': '345.82', 'w': '8.74'}, {'page': '10', 'x': '134.77', 'y': '354.35', 'h': '345.83', 'w': '8.74'}, {'page': '10', 'x': '134.77', 'y': '366.30', 'h': '118.83', 'w': '8.74'}], [{'page': '10', 'x': '256.29', 'y': '366.30', 'h': '224.30', 'w': '8.74'}]]\", 'pages': \"('10', '10')\", 'section_title': 'Few-Shot Image-Conditioned Detection Performance', 'section_number': '4.4', 'paper_title': 'Simple Open-Vocabulary Object Detection with Vision Transformers', 'file_path': '../data/open_vocab_vit_object_detection.pdf'}, page_content='Table 2. One-and few-shot image-conditioned detection performance on COCO AP50.Our method (R50+H/32 architecture) strongly outperforms prior work and also shows marked improvements as the number of conditioning queries is increased to k = 10.COCO category splits as in [16].Because the evaluation is stochastic, for our results, we report the average across 3 runs.For evaluation on this task, we follow the procedure described in [16]: During detection training, we hold out some COCO categories to evaluate on, and in addition all synonymous and semantically descendant categories that appear in our detection training data.We do not modify the image-text pre-training stage.')]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.invoke(\"What is open vocabulory detection as explained in the paper?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshay/miniconda3/envs/vlm/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "/home/akshay/miniconda3/envs/vlm/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:784: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={'pages': \"('18', '18')\", 'para': '2', 'section_number': 'None', 'section_title': 'Appendix'}, page_content='The appendix provides additional examples, results and methodological details.For remaining questions, please refer to the code at github.com/google-research/scenic/tree/main/scenic/projects/owl_vit.'), -237.82799116487467), (Document(metadata={'pages': \"('18', '18')\", 'para': '2', 'section_number': 'None', 'section_title': 'Appendix'}, page_content='The appendix provides additional examples, results and methodological details.For remaining questions, please refer to the code at github.com/google-research/scenic/tree/main/scenic/projects/owl_vit.'), -237.82799116487467), (Document(metadata={'pages': \"('19', '19')\", 'para': '3', 'section_number': 'None', 'section_title': 'A1.2 Detection Datasets'}, page_content='Visual Genome (VG) [23] contains dense annotations for objects, regions, object attributes, and their relationships within each image.VG is based on COCO images, which are re-annotated with free-text annotations for an average of 35 objects per image.All entities are canonicalized to WordNet synsets.We only use object annotations from this dataset, and do not train models using the attribute, relationship or region annotations.'), -260.9955136201903), (Document(metadata={'pages': \"('25', '25')\", 'para': '9', 'section_number': 'None', 'section_title': 'A1.9 Extended Ablation Study'}, page_content='Dataset ratios.In the majority of our experiments we use OI and VG datasets for training.In the ablation study presented in the main text (Table 3), we showed that having more training data (i.e.training on both VG and OI) improves zero-shot performance.Here, we further explored the optimal ratio in which these datasets should be mixed and found that a 7:3 = OI:VG ratio worked best.Note that this overweighs VG significantly compared to the relative size of these datasets.Overweighing VG might be beneficial because VG has a larger label space than OI, such that each VG example provides more valuable semantic supervision than each OI example.We also tested the relative value of VG \"object\" and \"region\" annotations.In VG, \"region\" annotations provide free-text descriptions of whole image regions, as opposed to the standard single-object annotations.Interestingly, we found that training on the region annotations hurts the generalization ability of our models, so we do not use them for training.'), -261.5815827446485)]\n",
      "  warnings.warn(\n",
      "/home/akshay/miniconda3/envs/vlm/lib/python3.12/site-packages/langchain_core/vectorstores/base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.8\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the Abstract and Title of the paper?. Refer Abstract section.\"\n",
    "result = qa({\"query\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the Abstract and Title of the paper?. Refer Abstract section.',\n",
       " 'result': \"Unfortunately, I don't have enough information to provide the title and abstract of the paper. The provided text appears to be a snippet from the introduction or methodology section of the paper, but it doesn't contain the title and abstract.\\n\\nHowever, based on the content, I can infer that the paper is likely related to computer vision and object detection, possibly using transformer-based architectures. If you have access to the full paper, I'd be happy to help with any other questions!\",\n",
       " 'source_documents': [Document(metadata={'para': '6', 'pages': \"('5', '5')\", 'section_title': 'Model', 'section_number': '3.1'}, page_content=\"Architecture.Our model uses a standard Vision Transformer as the image encoder and a similar Transformer architecture as the text encoder (Figure 1).To adapt the image encoder for detection, we remove the token pooling and final projection layer, and instead linearly project each output token representation to obtain per-object image embeddings for classification (Figure 1, right).The maximum number of predicted objects is therefore equal to the number of tokens (sequence length) of the image encoder.This is not a bottleneck in practice since the sequence length of our models is at least 576 (ViT-B/32 at input size 768 Ã— 768), which is larger than the maximum number of instances in today's datasets (e.g., 294 instances for LVIS [13]).Box coordinates are obtained by passing token representations through a small MLP.Our setup resembles DETR [6], but is simplified by removing the decoder.\"),\n",
       "  Document(metadata={'para': '1', 'pages': \"('6', '6')\", 'section_title': 'Training', 'section_number': '3.2'}, page_content=\"An advantage of our encoder-only architecture is that nearly all of the model's parameters (image and text encoder) can benefit from image-level pre-training.The detection-specific heads contain at most 1.1% (depending on the model size) of the parameters of the model.\"),\n",
       "  Document(metadata={'para': '6', 'pages': \"('4', '4')\", 'section_title': 'Related Work', 'section_number': '2'}, page_content='Image-Conditioned Detection.Related to open-vocabulary detection is the task of image-conditioned detection, which refers to the ability to detect objects matching a single query image which shows an object of the category in question [4,16,7,31].This task is also called one-shot object detection because the query image is essentially a single training example.Image-based querying allows openworld detection when even the name of the object is unknown, e.g. for unique objects or specialized technical parts.Our model can perform this task without modifications by simply using image-derived instead of text-derived embeddings as queries.Recent prior works on this problem have focused mainly on architectural innovations, for example using sophisticated forms of cross-attention between the query and target image [16,7].Our approach instead relies on a simple but large model and extensive image-text pre-training.')]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
