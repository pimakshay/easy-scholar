{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying from a pdf\n",
    "\n",
    "- In this notebook, we load a random pdf and try to query from it.\n",
    "- We use ollama open-source models: `gemma2:2b` or `llama3.1:8b`\n",
    "- Embedding model: `nomic-embed-text`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama commands [Linux]\n",
    "\n",
    "Starting and stopping service\n",
    "1. Starting ollama service: `systemctl start ollama.service`\n",
    "2. Stopping ollama service: `systemctl stop ollama.service`\n",
    "3. Status of ollama service: `systemctl status ollama.service`\n",
    "\n",
    "Loading models\n",
    "1. pull the gemma2:2.b model:  `ollama pull gemma2:2b`\n",
    "2. run gemma model: `ollama run gemma2:2b`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Embedding model\n",
    "\n",
    "1. Install ollama\n",
    "2. Pull embedding model: `ollama pull nomic-embed-text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=\"../data/open_vocab_vit_object_detection.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docSearch = Chroma.from_documents(texts, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms.ollama import Ollama\n",
    "llm_model = Ollama(\n",
    "    model=\"gemma2:2b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm_model, chain_type=\"stuff\", retriever=docSearch.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The paper proposes a novel architecture for object detection and transfer learning, focusing on leveraging strong pre-training by combining text and image encoders. Here\\'s a breakdown of key aspects:\\n\\n**Key Ideas:**\\n\\n* **Encoder-only Architecture:**  The model relies solely on text and image encoders without relying on fusion techniques between them. This simplifies the architecture and allows for more efficient training.\\n* **Image-Level Contrastive Pre-Training:** The models are pre-trained using contrastive learning, leveraging a large image and text dataset to learn general representations.  This provides robustness and avoids overfitting during fine-tuning. \\n* **Fine-Tuning with Queries:** Instead of relying on textual embeddings for object descriptions, the model works directly with images as queries in the classification head. This is especially beneficial for objects that are hard to describe with words.\\n\\n**Technical Details:**\\n\\n* **Freezing Text Encoder (partially):**  Freezing the text encoder during fine-tuning helps prevent \"forgetting\" of learned semantic information from pre-training, potentially leading to better results.\\n* **Biased Box Coordinates:** Centering predicted box coordinates at the position of corresponding tokens on a 2D grid improves learning speed and performance by breaking symmetry during bipartite matching (the process used for loss calculations).  \\n* **Stochastic Depth Regularization:** To mitigate overfitting, stochastic depth regularization is applied to both image and text encoders.\\n* **Focal Sigmoid Cross-Entropy Loss:** This type of loss function addresses the challenge of long-tailed datasets and effectively handles scenarios with imbalanced classes. \\n\\n**Advantages & Impact:**\\n\\n* **Transfer Learning on Open Vocabulary:**  The approach enables object detection using only image data, making it applicable to situations with a broad vocabulary of objects (objects not explicitly labeled in text).\\n* **Reduced Data Requirements:** The model performs well even with relatively limited training data due to its ability to leverage pre-training and transfer learning.\\n\\n**Overall, the paper presents a promising framework for object detection using a novel encoder-based architecture that leverages large pre-trained models and transfer learning techniques.** \\n\\n\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Summary the pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The paper proposes a novel architecture for object detection and transfer '\n",
      " 'learning, focusing on leveraging strong pre-training by combining text and '\n",
      " \"image encoders. Here's a breakdown of key aspects:\\n\"\n",
      " '\\n'\n",
      " '**Key Ideas:**\\n'\n",
      " '\\n'\n",
      " '* **Encoder-only Architecture:**  The model relies solely on text and image '\n",
      " 'encoders without relying on fusion techniques between them. This simplifies '\n",
      " 'the architecture and allows for more efficient training.\\n'\n",
      " '* **Image-Level Contrastive Pre-Training:** The models are pre-trained using '\n",
      " 'contrastive learning, leveraging a large image and text dataset to learn '\n",
      " 'general representations.  This provides robustness and avoids overfitting '\n",
      " 'during fine-tuning. \\n'\n",
      " '* **Fine-Tuning with Queries:** Instead of relying on textual embeddings for '\n",
      " 'object descriptions, the model works directly with images as queries in the '\n",
      " 'classification head. This is especially beneficial for objects that are hard '\n",
      " 'to describe with words.\\n'\n",
      " '\\n'\n",
      " '**Technical Details:**\\n'\n",
      " '\\n'\n",
      " '* **Freezing Text Encoder (partially):**  Freezing the text encoder during '\n",
      " 'fine-tuning helps prevent \"forgetting\" of learned semantic information from '\n",
      " 'pre-training, potentially leading to better results.\\n'\n",
      " '* **Biased Box Coordinates:** Centering predicted box coordinates at the '\n",
      " 'position of corresponding tokens on a 2D grid improves learning speed and '\n",
      " 'performance by breaking symmetry during bipartite matching (the process used '\n",
      " 'for loss calculations).  \\n'\n",
      " '* **Stochastic Depth Regularization:** To mitigate overfitting, stochastic '\n",
      " 'depth regularization is applied to both image and text encoders.\\n'\n",
      " '* **Focal Sigmoid Cross-Entropy Loss:** This type of loss function addresses '\n",
      " 'the challenge of long-tailed datasets and effectively handles scenarios with '\n",
      " 'imbalanced classes. \\n'\n",
      " '\\n'\n",
      " '**Advantages & Impact:**\\n'\n",
      " '\\n'\n",
      " '* **Transfer Learning on Open Vocabulary:**  The approach enables object '\n",
      " 'detection using only image data, making it applicable to situations with a '\n",
      " 'broad vocabulary of objects (objects not explicitly labeled in text).\\n'\n",
      " '* **Reduced Data Requirements:** The model performs well even with '\n",
      " 'relatively limited training data due to its ability to leverage pre-training '\n",
      " 'and transfer learning.\\n'\n",
      " '\\n'\n",
      " '**Overall, the paper presents a promising framework for object detection '\n",
      " 'using a novel encoder-based architecture that leverages large pre-trained '\n",
      " 'models and transfer learning techniques.** \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint('The paper proposes a novel architecture for object detection and transfer learning, focusing on leveraging strong pre-training by combining text and image encoders. Here\\'s a breakdown of key aspects:\\n\\n**Key Ideas:**\\n\\n* **Encoder-only Architecture:**  The model relies solely on text and image encoders without relying on fusion techniques between them. This simplifies the architecture and allows for more efficient training.\\n* **Image-Level Contrastive Pre-Training:** The models are pre-trained using contrastive learning, leveraging a large image and text dataset to learn general representations.  This provides robustness and avoids overfitting during fine-tuning. \\n* **Fine-Tuning with Queries:** Instead of relying on textual embeddings for object descriptions, the model works directly with images as queries in the classification head. This is especially beneficial for objects that are hard to describe with words.\\n\\n**Technical Details:**\\n\\n* **Freezing Text Encoder (partially):**  Freezing the text encoder during fine-tuning helps prevent \"forgetting\" of learned semantic information from pre-training, potentially leading to better results.\\n* **Biased Box Coordinates:** Centering predicted box coordinates at the position of corresponding tokens on a 2D grid improves learning speed and performance by breaking symmetry during bipartite matching (the process used for loss calculations).  \\n* **Stochastic Depth Regularization:** To mitigate overfitting, stochastic depth regularization is applied to both image and text encoders.\\n* **Focal Sigmoid Cross-Entropy Loss:** This type of loss function addresses the challenge of long-tailed datasets and effectively handles scenarios with imbalanced classes. \\n\\n**Advantages & Impact:**\\n\\n* **Transfer Learning on Open Vocabulary:**  The approach enables object detection using only image data, making it applicable to situations with a broad vocabulary of objects (objects not explicitly labeled in text).\\n* **Reduced Data Requirements:** The model performs well even with relatively limited training data due to its ability to leverage pre-training and transfer learning.\\n\\n**Overall, the paper presents a promising framework for object detection using a novel encoder-based architecture that leverages large pre-trained models and transfer learning techniques.** \\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are the top 3 references based on their relevance to the provided text, along with a brief explanation of why they\\'re important:\\n\\n1. **[25,43]** This is most likely referring to papers that describe \"head attention pooling\" and its usage in object detection models. It highlights the core aggregation technique used in the proposed approach. \\n2. **[33]**, **[39,38,3]**  This references a well-established set of works on \"fine-tuning for classification\", specifically in the context of large Transformer models. These resources offer valuable insights into the practical aspects of training these types of models.\\n3. **[6], [13,24], [47]**, These are references to papers that detail specific techniques used for object detection, such as DETR\\'s bipartite matching loss and federated annotation methods. This demonstrates how they address challenges related to open-vocabulary detection datasets, which require efficient data management and unique training approaches. \\n\\n\\n\\n**Explanation:**\\n\\n* **The text emphasizes a new approach for open-vocabulary object detection.**  \\n* The references provide crucial context on: \\n    * **Model architectures:** Head attention pooling, pre-trained models.\\n    * **Training techniques:** Fine-tuning methods, federated annotation, prompt engineering.\\n    * **Specific challenges:** Biases for location prediction, handling open vocabulary in datasets.\\n\\nLet me know if you\\'d like more explanation about any particular reference or aspect of the text! \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(\"Extract top 3 references from the 'References' section of the pdf which explains 80 percentage of the information in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Here are the top 3 references based on their relevance to the provided text, '\n",
      " \"along with a brief explanation of why they're important:\\n\"\n",
      " '\\n'\n",
      " '1. **[25,43]** This is most likely referring to papers that describe \"head '\n",
      " 'attention pooling\" and its usage in object detection models. It highlights '\n",
      " 'the core aggregation technique used in the proposed approach. \\n'\n",
      " '2. **[33]**, **[39,38,3]**  This references a well-established set of works '\n",
      " 'on \"fine-tuning for classification\", specifically in the context of large '\n",
      " 'Transformer models. These resources offer valuable insights into the '\n",
      " 'practical aspects of training these types of models.\\n'\n",
      " '3. **[6], [13,24], [47]**, These are references to papers that detail '\n",
      " \"specific techniques used for object detection, such as DETR's bipartite \"\n",
      " 'matching loss and federated annotation methods. This demonstrates how they '\n",
      " 'address challenges related to open-vocabulary detection datasets, which '\n",
      " 'require efficient data management and unique training approaches. \\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '**Explanation:**\\n'\n",
      " '\\n'\n",
      " '* **The text emphasizes a new approach for open-vocabulary object '\n",
      " 'detection.**  \\n'\n",
      " '* The references provide crucial context on: \\n'\n",
      " '    * **Model architectures:** Head attention pooling, pre-trained models.\\n'\n",
      " '    * **Training techniques:** Fine-tuning methods, federated annotation, '\n",
      " 'prompt engineering.\\n'\n",
      " '    * **Specific challenges:** Biases for location prediction, handling open '\n",
      " 'vocabulary in datasets.\\n'\n",
      " '\\n'\n",
      " \"Let me know if you'd like more explanation about any particular reference or \"\n",
      " 'aspect of the text! \\n')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint('Here are the top 3 references based on their relevance to the provided text, along with a brief explanation of why they\\'re important:\\n\\n1. **[25,43]** This is most likely referring to papers that describe \"head attention pooling\" and its usage in object detection models. It highlights the core aggregation technique used in the proposed approach. \\n2. **[33]**, **[39,38,3]**  This references a well-established set of works on \"fine-tuning for classification\", specifically in the context of large Transformer models. These resources offer valuable insights into the practical aspects of training these types of models.\\n3. **[6], [13,24], [47]**, These are references to papers that detail specific techniques used for object detection, such as DETR\\'s bipartite matching loss and federated annotation methods. This demonstrates how they address challenges related to open-vocabulary detection datasets, which require efficient data management and unique training approaches. \\n\\n\\n\\n**Explanation:**\\n\\n* **The text emphasizes a new approach for open-vocabulary object detection.**  \\n* The references provide crucial context on: \\n    * **Model architectures:** Head attention pooling, pre-trained models.\\n    * **Training techniques:** Fine-tuning methods, federated annotation, prompt engineering.\\n    * **Specific challenges:** Biases for location prediction, handling open vocabulary in datasets.\\n\\nLet me know if you\\'d like more explanation about any particular reference or aspect of the text! \\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
